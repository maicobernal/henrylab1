{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://sigmundojr.medium.com/how-do-i-read-a-csv-file-from-google-drive-using-python-colab-966091922852\n",
    "\n",
    "#https://drive.google.com/drive/folders/1Rsq-HHomPtQwy7RIWQ574wKcf56LiGq1\n",
    "\n",
    "\n",
    "\n",
    "\"\"\" from pydrive.auth import GoogleAuth\n",
    "\n",
    "from pydrive.drive import GoogleDrive\n",
    "\n",
    "from google.colab import auth\n",
    "\n",
    "from oauth2client.client import GoogleCredentials\n",
    "\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "import os\n",
    "import glob\n",
    "\n",
    "spacer = '*'*10\n",
    "path = './datasets/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import a single file, \n",
    "# name = filename\n",
    "# tipo = extension file,\n",
    "# path = path to file, \n",
    "# spacer = separator for CSV/TXT\n",
    "# encoding = encoding for CSV/TXT\n",
    "\n",
    "def FileImporter (name: str, tipo: str, spacer:str = ',', path:str = path, encoding:str = 'utf-8', sheet:int = 0):\n",
    "\n",
    "    #Raise and error if type of file is not declared\n",
    "    if tipo == '':\n",
    "        raise ValueError ('You need to put some extension ir order to import the file')\n",
    "\n",
    "    #Set the path to the file and extension\n",
    "    file = path + name + '.' + tipo\n",
    "    \n",
    "    #DEBUG\n",
    "    #print(file)\n",
    "    \n",
    "    try:\n",
    "        #CSV with encoding error\n",
    "        if tipo == 'csv':\n",
    "            try:\n",
    "                df = pd.read_csv(file, sep=spacer, encoding=encoding, low_memory=False)\n",
    "                return df\n",
    "            except UnicodeDecodeError as e:\n",
    "                print('Try a different encoding method for the file', e)\n",
    "        #XLS/XLSX\n",
    "        elif tipo == 'xls' or tipo == 'xlsx':\n",
    "            df = pd.read_excel(file, sheet_name = sheet)\n",
    "            return df\n",
    "        \n",
    "        #JSON\n",
    "        elif tipo == 'json':\n",
    "            df = pd.read_json(file)\n",
    "            return df\n",
    "\n",
    "        #TXT\n",
    "        elif tipo == 'txt':\n",
    "            df = pd.read_csv(file, sep=spacer, encoding='utf-8')\n",
    "            return df\n",
    "\n",
    "        #PARQUET\n",
    "        elif tipo == 'parquet':\n",
    "            df = pd.read_parquet(file)\n",
    "            return df\n",
    "            \n",
    "    except FileNotFoundError as f:\n",
    "        print('Error reading file' + str(f))\n",
    "\n",
    "    finally:\n",
    "        print('Importing successfully done for ', file)\n",
    "\n",
    "\n",
    "#Import all files in a folder, path = path to folder, spacer = separator for CSV/TXT\n",
    "def FolderImporter (path:str = path, spacer:str = ','):\n",
    "    all_files = glob.glob(path + \"/*.csv\")\n",
    "    li = []\n",
    "    for filename in all_files:\n",
    "        df = pd.read_csv(filename, index_col=None, header=0, sep=spacer, encoding='utf-8')\n",
    "        li.append(df)\n",
    "    frame = pd.concat(li, axis=0, ignore_index=True)\n",
    "    return frame\n",
    "\n",
    "#Normalize strings and encoding for each column\n",
    "def NormalizeColumn(df, column_name):\n",
    "    df[column_name] = df[column_name].str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')\n",
    "    return df[column_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ETL for producto\n",
    "delete_columns = ['categoria1', 'categoria2', 'categoria3']\n",
    "\n",
    "def CleanProducto(df):\n",
    "    df.drop(columns=delete_columns, inplace=True)\n",
    "    df['nombre'] = NormalizeColumn(df, 'nombre')\n",
    "    df['presentacion'] = NormalizeColumn(df, 'presentacion')\n",
    "    df['marca'] = NormalizeColumn(df, 'marca')\n",
    "    df['id'] = NormalizeColumn(df, 'id')\n",
    "    df['id'] = df['id'].str.replace('-', '').astype(int)\n",
    "    df['nombre'] = df['nombre'].str.split(r\"\\s\\d*\\s\", regex=True, expand=False).str[0].str.upper()\n",
    "    return df\n",
    "\n",
    "def CleanSucursal(df):\n",
    "    try:\n",
    "        df['id'] = df['id'].str.replace('-', '').astype(int)\n",
    "    except:\n",
    "        print('id already cleaned')\n",
    "        pass\n",
    "    #df['sucursalId'] = df['id'].str.split('-', regex=False, expand=False).str[2]\n",
    "    df['banderaDescripcion'] = NormalizeColumn(df, 'banderaDescripcion').str.upper()\n",
    "    df['comercioRazonSocial'] = NormalizeColumn(df, 'comercioRazonSocial').str.upper()\n",
    "    df['localidad'] = NormalizeColumn(df, 'localidad').str.upper()\n",
    "    df['direccion'] = NormalizeColumn(df, 'direccion').str.upper()\n",
    "    return df\n",
    "\n",
    "\n",
    "def CleanPrecios(df):\n",
    "    #Set order of columns\n",
    "    col_order = ['precio', 'sucursal_id', 'producto_id']\n",
    "\n",
    "    #Get percentage of null values for each columns and return it in a list\n",
    "    checkna = df.isna().sum().div(df.shape[0]).mul(100).round(3).tolist()\n",
    "\n",
    "    #If NA <1% then drop the column else raise an error\n",
    "    for i in checkna:\n",
    "        if i < 1:\n",
    "            print('Not many null values less than 1%')\n",
    "            df.dropna(inplace=True)\n",
    "            break\n",
    "        else:\n",
    "            raise ValueError('There are too many null values in the dataset, check it')\n",
    "\n",
    "    #Clean sucursal_id and keep only real sucursal ID          \n",
    "    try: \n",
    "        df['sucursal_id'] = df['sucursal_id'].str.replace('-', '').astype(int)       \n",
    "        #df['sucursal_id'] = df['sucursal_id'].str.split('-', regex=False, expand=False).str[2].astype(int)\n",
    "    except:\n",
    "        print('sucursal_id already cleaned')\n",
    "        pass\n",
    "\n",
    "    #Clean producto ID\n",
    "    try:\n",
    "        df['producto_id'] = df['producto_id'].str.replace('-', '').astype(int)\n",
    "    except:\n",
    "        print('producto_id already cleaned')\n",
    "        pass\n",
    "\n",
    "    #Clean precio\n",
    "    df['precio'] = df['precio'].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    return df[col_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing successfully done for  ./datasets/producto.parquet\n",
      "Importing successfully done for  ./datasets/sucursal.csv\n",
      "Importing successfully done for  ./datasets/precios_semana_20200413.csv\n",
      "Not many null values less than 1%\n",
      "Importing successfully done for  ./datasets/precios_semanas_20200419_20200426.xlsx\n",
      "Not many null values less than 1%\n",
      "producto_id already cleaned\n",
      "Importing successfully done for  ./datasets/precios_semanas_20200419_20200426.xlsx\n",
      "Not many null values less than 1%\n",
      "sucursal_id already cleaned\n",
      "producto_id already cleaned\n",
      "Importing successfully done for  ./datasets/precios_semana_20200503.json\n",
      "Not many null values less than 1%\n",
      "producto_id already cleaned\n",
      "Importing successfully done for  ./datasets/precios_semana_20200518.txt\n",
      "Not many null values less than 1%\n"
     ]
    }
   ],
   "source": [
    "# Import files locally\n",
    "producto = CleanProducto(FileImporter('producto', 'parquet'))\n",
    "sucursal = CleanSucursal(FileImporter('sucursal', 'csv'))\n",
    "precioW1 = CleanPrecios(FileImporter('precios_semana_20200413', tipo = 'csv', encoding = 'utf-16'))\n",
    "precioW2 = CleanPrecios(FileImporter('precios_semanas_20200419_20200426', 'xlsx', sheet = 0))\n",
    "precioW3 = CleanPrecios(FileImporter('precios_semanas_20200419_20200426', 'xlsx', sheet = 1))\n",
    "precioW4 = CleanPrecios(FileImporter('precios_semana_20200503', 'json'))\n",
    "precioW5 = CleanPrecios(FileImporter('precios_semana_20200518', 'txt', spacer = '|'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "precio_final = pd.concat([precioW1, precioW2, precioW3, precioW4, precioW5], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2203705"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the module\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# create sqlalchemy engine\n",
    "engine = create_engine(\"mysql+pymysql://{user}:{pw}@localhost/{db}\"\n",
    "                       .format(user=\"pythonuser\",\n",
    "                               pw=\"borito333.\",\n",
    "                               db=\"lab1\"))\n",
    "producto.to_sql('producto', engine, if_exists='append', index=False)\n",
    "sucursal.to_sql('sucursal', engine, if_exists='append', index=False)\n",
    "precio_final.to_sql('precio_final', engine, if_exists='append', index=False)\n",
    "#nuevos_precios.to_sql('precio_final', engine, if_exists='append', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>avg(p.precio)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>199.703194</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   avg(p.precio)\n",
       "0     199.703194"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query ='''select avg(p.precio) from sucursal as s\n",
    "join precio_final as p on (s.id = p.sucursal_id)\n",
    "where s.id = '91688';'''\n",
    "\n",
    "pd.read_sql_query(query, engine)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "11f276e131df8708bc2fc0bb3682099dca2cbd19e2af230e0a94f818ba1c6df6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
